## Optimized Masked Autoencoders (MAE)

An optimized implementation of masked autoencoders (MAEs). The following optimizations are planned to be implemented:

- [ ] FlashAttention-2
- [ ] `torch.compile`
- [ ] optimized AdamW (`foreach` and `fused`)
- [ ] `FSDP` for distributed training